"""
RAG System para Consultas de Buenas PrÃ¡cticas Ganaderas (BPG)
Arquitectura: ChromaDB (vectores) + Ollama (generaciÃ³n)
Autor: Sistema RAG BPG
VersiÃ³n: 2.0 - Con Sistema de ConfiguraciÃ³n
"""

import os
import sys
from typing import List, Dict, Tuple, Optional
import chromadb
from sentence_transformers import SentenceTransformer
import requests
import json
from datetime import datetime

# âœ¨ NUEVO: Importar sistema de configuraciÃ³n
try:
    from config.settings import RAGConfig, DEFAULT_CONFIG
    CONFIG_AVAILABLE = True
except ImportError:
    CONFIG_AVAILABLE = False
    print("âš ï¸  Sistema de configuraciÃ³n no disponible, usando parÃ¡metros legacy")


class RAGSystemBPG:
    """Sistema RAG completo para consultas sobre Buenas PrÃ¡cticas Ganaderas"""
    
    def __init__(
        self, 
        # âœ¨ NUEVO: ParÃ¡metro principal de configuraciÃ³n
        config: Optional['RAGConfig'] = None,
        
        # Legacy parameters (para compatibilidad hacia atrÃ¡s)
        chroma_db_path: Optional[str] = None,
        embedding_model_name: Optional[str] = None,
        ollama_base_url: Optional[str] = None,
        ollama_model: Optional[str] = None
    ):
        """
        Inicializar sistema RAG con configuraciÃ³n flexible
        
        Args:
            config: Objeto RAGConfig (recomendado) - Si se proporciona, ignora otros parÃ¡metros
            chroma_db_path: [LEGACY] Ruta a ChromaDB
            embedding_model_name: [LEGACY] Modelo de embeddings
            ollama_base_url: [LEGACY] URL de Ollama
            ollama_model: [LEGACY] Modelo de Ollama
            
        Ejemplos:
            # Forma nueva (recomendada):
            >>> from config.settings import RAGConfig
            >>> config = RAGConfig(ollama_model="mistral")
            >>> rag = RAGSystemBPG(config=config)
            
            # Forma antigua (sigue funcionando):
            >>> rag = RAGSystemBPG(ollama_model="llama3.2")
        """
        print("ğŸš€ Inicializando Sistema RAG BPG...")
        
        # ===== MANEJO DE CONFIGURACIÃ“N =====
        if config is not None:
            # Usar configuraciÃ³n nueva
            self.config = config
            print("âœ… Usando configuraciÃ³n centralizada")
        elif CONFIG_AVAILABLE:
            # Crear config desde parÃ¡metros legacy
            config_kwargs = {}
            if chroma_db_path is not None:
                config_kwargs['chroma_db_path'] = chroma_db_path
            if embedding_model_name is not None:
                config_kwargs['embedding_model'] = embedding_model_name
            if ollama_base_url is not None:
                config_kwargs['ollama_base_url'] = ollama_base_url
            if ollama_model is not None:
                config_kwargs['ollama_model'] = ollama_model
            
            if config_kwargs:
                self.config = RAGConfig(**config_kwargs)
                print("âœ… ConfiguraciÃ³n creada desde parÃ¡metros legacy")
            else:
                self.config = DEFAULT_CONFIG
                print("âœ… Usando configuraciÃ³n por defecto")
        else:
            # Fallback: usar parÃ¡metros directos (modo legacy puro)
            print("âš ï¸  Modo legacy: sin sistema de configuraciÃ³n")
            self.chroma_db_path = chroma_db_path or "models/chroma_db"
            self.embedding_model_name = embedding_model_name or "paraphrase-multilingual-mpnet-base-v2"
            self.ollama_base_url = ollama_base_url or "http://localhost:11434"
            self.ollama_model = ollama_model or "llama3.2"
            self.config = None
        
        # ===== EXTRAER PARÃMETROS DE CONFIG =====
        if self.config:
            self.chroma_db_path = self.config.chroma_db_path
            self.embedding_model_name = self.config.embedding_model
            self.ollama_base_url = self.config.ollama_base_url
            self.ollama_model = self.config.ollama_model
        
        # ===== INICIALIZACIÃ“N (igual que antes) =====
        # Cargar modelo de embeddings
        print(f"ğŸ“¦ Cargando modelo de embeddings: {self.embedding_model_name}")
        self.embedding_model = SentenceTransformer(self.embedding_model_name)
        print("âœ… Modelo de embeddings cargado")
        
        # Conectar a ChromaDB
        print(f"ğŸ—„ï¸  Conectando a ChromaDB: {self.chroma_db_path}")
        self.chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)
        
        # Usar collection_name de config si estÃ¡ disponible
        collection_name = self.config.collection_name if self.config else "bpg_manuals"
        self.collection = self.chroma_client.get_collection(name=collection_name)
        print(f"âœ… Conectado a ChromaDB - {self.collection.count()} documentos disponibles")
        
        # Verificar conexiÃ³n con Ollama
        self._verificar_ollama()
        
        print("âœ… Sistema RAG inicializado correctamente\n")
    
    def _verificar_ollama(self):
        """Verificar que Ollama estÃ© corriendo y el modelo disponible"""
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags")
            if response.status_code == 200:
                modelos = response.json().get('models', [])
                modelos_nombres = [m['name'] for m in modelos]
                print(f"âœ… Ollama conectado - Modelos disponibles: {modelos_nombres}")
                
                if self.ollama_model not in modelos_nombres:
                    print(f"âš ï¸  ADVERTENCIA: Modelo '{self.ollama_model}' no encontrado")
                    print(f"   Ejecuta: ollama pull {self.ollama_model}")
            else:
                print(f"âŒ Error conectando a Ollama (status: {response.status_code})")
        except Exception as e:
            print(f"âŒ Error: Ollama no estÃ¡ corriendo en {self.ollama_base_url}")
            print(f"   Inicia Ollama con: ollama serve")
            print(f"   Error detallado: {str(e)}")
    
    def retrieve_documents(
        self, 
        query: str, 
        k: int = 5,
        min_similarity: float = 0.0
    ) -> List[Dict]:
        """
        Recuperar documentos relevantes de ChromaDB
        
        Args:
            query: Pregunta del usuario
            k: NÃºmero de chunks a recuperar
            min_similarity: Similaridad mÃ­nima (0-1)
            
        Returns:
            Lista de documentos relevantes con metadata
        """
        print(f"\nğŸ” Buscando documentos relevantes para: '{query}'")
        
        # Generar embedding de la query
        query_embedding = self.embedding_model.encode([query])[0].tolist()
        
        # Buscar en ChromaDB
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=k
        )
        
        # Procesar resultados
        documentos_relevantes = []
        if results['documents'] and results['documents'][0]:
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            )):
                # Convertir distancia a similaridad (ChromaDB usa distancia L2)
                similarity = 1 / (1 + distance)
                
                if similarity >= min_similarity:
                    documentos_relevantes.append({
                        'rank': i + 1,
                        'text': doc,
                        'metadata': metadata,
                        'similarity': round(similarity, 4),
                        'distance': round(distance, 4)
                    })
        
        print(f"âœ… Recuperados {len(documentos_relevantes)} documentos relevantes")
        return documentos_relevantes
    
    def generate_answer(
        self, 
        query: str, 
        context_docs: List[Dict],
        temperature: float = 0.7,
        max_tokens: int = 500
    ) -> Dict:
        """
        Generar respuesta usando Ollama con contexto recuperado
        
        Args:
            query: Pregunta del usuario
            context_docs: Documentos recuperados del retriever
            temperature: Creatividad del modelo (0-1)
            max_tokens: MÃ¡ximo de tokens en respuesta
            
        Returns:
            Diccionario con respuesta y metadata
        """
        print(f"\nğŸ¤– Generando respuesta con Ollama ({self.ollama_model})...")
        
        # Construir contexto
        context = "\n\n---\n\n".join([
            f"Fragmento {doc['rank']} (Similaridad: {doc['similarity']}):\n{doc['text']}"
            for doc in context_docs
        ])
        
        # Construir prompt optimizado para BPG
        prompt = f"""Eres un experto en Buenas PrÃ¡cticas Ganaderas (BPG) para ganado vacuno de carne. 

Tu tarea es responder preguntas de productores ganaderos basÃ¡ndote ÃšNICAMENTE en la informaciÃ³n proporcionada en los documentos de referencia.

DOCUMENTOS DE REFERENCIA:
{context}

PREGUNTA DEL PRODUCTOR:
{query}

INSTRUCCIONES:
1. Responde SOLO con informaciÃ³n de los documentos de referencia
2. Si la informaciÃ³n no estÃ¡ en los documentos, di "No tengo informaciÃ³n suficiente en los manuales"
3. SÃ© especÃ­fico, prÃ¡ctico y directo
4. Usa un lenguaje profesional pero accesible
5. Si hay normativas o nÃºmeros especÃ­ficos, cÃ­talos exactamente

RESPUESTA:"""
        
        # Llamar a Ollama API
        try:
            response = requests.post(
                f"{self.ollama_base_url}/api/generate",
                json={
                    "model": self.ollama_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_predict": max_tokens
                    }
                },
                timeout=120  # 2 minutos timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                answer_text = result.get('response', '').strip()
                
                print(f"âœ… Respuesta generada ({len(answer_text)} caracteres)")
                
                return {
                    'answer': answer_text,
                    'model': self.ollama_model,
                    'num_docs_used': len(context_docs),
                    'total_eval_duration': result.get('total_duration', 0) / 1e9,  # nanosegundos a segundos
                    'success': True
                }
            else:
                error_msg = f"Error de Ollama (status {response.status_code})"
                print(f"âŒ {error_msg}")
                return {
                    'answer': error_msg,
                    'success': False
                }
                
        except Exception as e:
            error_msg = f"Error al generar respuesta: {str(e)}"
            print(f"âŒ {error_msg}")
            return {
                'answer': error_msg,
                'success': False
            }
    
    def query(
        self, 
        pregunta: str, 
        k: int = 5,
        temperature: float = 0.7,
        verbose: bool = True
    ) -> Dict:
        """
        Ejecutar consulta completa RAG (Retrieve + Generate)
        
        Args:
            pregunta: Pregunta del usuario
            k: NÃºmero de chunks a recuperar
            temperature: Creatividad de la respuesta
            verbose: Mostrar documentos recuperados
            
        Returns:
            Respuesta completa con metadata
        """
        print("\n" + "="*60)
        print("ğŸ“‹ CONSULTA RAG BPG")
        print("="*60)
        
        # Usar valores de configuraciÃ³n si estÃ¡n disponibles
        if self.config:
            k = k or self.config.default_k
            temperature = temperature or self.config.default_temperature
        
        # 1. RETRIEVAL
        docs_relevantes = self.retrieve_documents(pregunta, k=k)
        
        if verbose and docs_relevantes:
            print("\nğŸ“„ Documentos recuperados:")
            for doc in docs_relevantes[:3]:  # Mostrar top 3
                print(f"\n  Rank {doc['rank']} - Similaridad: {doc['similarity']}")
                print(f"  {doc['text'][:200]}...")
        
        # 2. GENERATION
        resultado = self.generate_answer(
            query=pregunta,
            context_docs=docs_relevantes,
            temperature=temperature
        )
        
        # Agregar informaciÃ³n de retrieval al resultado
        resultado['retrieved_docs'] = docs_relevantes
        resultado['query'] = pregunta
        
        return resultado
    
    def chat_interactivo(self):
        """Modo chat interactivo para pruebas"""
        print("\n" + "="*60)
        print("ğŸ’¬ CHAT INTERACTIVO RAG BPG")
        print("="*60)
        print("Escribe 'salir' para terminar\n")
        
        while True:
            try:
                pregunta = input("\nğŸ§‘â€ğŸŒ¾ Productor: ").strip()
                
                if pregunta.lower() in ['salir', 'exit', 'quit']:
                    print("ğŸ‘‹ Hasta luego!")
                    break
                
                if not pregunta:
                    continue
                
                resultado = self.query(pregunta, verbose=False)
                
                if resultado['success']:
                    print(f"\nğŸ¤– Asistente BPG: {resultado['answer']}")
                    print(f"\nâ±ï¸  Tiempo: {resultado['total_eval_duration']:.2f}s | Docs: {resultado['num_docs_used']}")
                else:
                    print(f"\nâŒ Error: {resultado['answer']}")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Hasta luego!")
                break
            except Exception as e:
                print(f"\nâŒ Error: {str(e)}")


def main():
    """FunciÃ³n principal para pruebas"""
    
    # âœ¨ NUEVO: Usar configuraciÃ³n si estÃ¡ disponible
    if CONFIG_AVAILABLE:
        print("\nğŸ¯ Usando sistema de configuraciÃ³n")
        rag = RAGSystemBPG(config=DEFAULT_CONFIG)
    else:
        print("\nâš ï¸  Modo legacy (sin configuraciÃ³n)")
        rag = RAGSystemBPG(
            chroma_db_path="models/chroma_db",
            embedding_model_name="paraphrase-multilingual-mpnet-base-v2",
            ollama_model="llama3.2"
        )
    
    # Ejemplos de consultas
    ejemplos = [
        "Â¿CuÃ¡les son las buenas prÃ¡cticas para el manejo del agua en feedlot?",
        "Â¿QuÃ© requisitos debe cumplir el establecimiento ganadero?",
        "Â¿CÃ³mo se debe manejar el bienestar animal durante el transporte?"
    ]
    
    print("\n" + "="*60)
    print("ğŸ§ª PROBANDO SISTEMA RAG CON CONSULTAS DE EJEMPLO")
    print("="*60)
    
    for i, pregunta in enumerate(ejemplos, 1):
        print(f"\n{'='*60}")
        print(f"EJEMPLO {i}")
        print(f"{'='*60}")
        
        resultado = rag.query(pregunta, k=3, temperature=0.7, verbose=True)
        
        if resultado['success']:
            print(f"\nğŸ“ RESPUESTA FINAL:\n{resultado['answer']}")
            print(f"\nâ±ï¸  Tiempo de generaciÃ³n: {resultado['total_eval_duration']:.2f} segundos")
        
        print("\n" + "-"*60)
    
    # Modo interactivo
    print("\n\nÂ¿Iniciar chat interactivo? (s/n): ", end="")
    if input().lower() == 's':
        rag.chat_interactivo()


if __name__ == "__main__":
    main()